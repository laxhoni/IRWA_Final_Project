{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f0e6a3",
   "metadata": {},
   "source": [
    "# Part 3. Ranking & Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bcee18",
   "metadata": {},
   "source": [
    "Author/s: <font color=\"blue\">Jhonatan Barcos Gambaro | Daniel Alexander Yearwood</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">jhonatan.barcos01@estudiant.upf.edu | danielalexander.yearwood01@estudiant.upf.edu </font>\n",
    "\n",
    "Date: <font color=\"blue\">20/11/2025</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c88deaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c9851",
   "metadata": {},
   "source": [
    "## 0. Data Preprocesing, Indexing and Queries (Recap Part 1-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5b0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "data_path = '../../data/fashion_products_dataset.json'\n",
    "products = pd.read_json(data_path)\n",
    "\n",
    "# Text Preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "# Define new stop words that depends on the domain of the data\n",
    "stop_words_domain = {\n",
    "    'made', 'india', 'proudly', 'use', 'year', 'round', \n",
    "    'look', 'design', 'qualiti', 'day', 'make',       \n",
    "    'feel', 'perfect', 'great', 'wash', 'style',      \n",
    "}\n",
    "stop_words = stop_words.union(stop_words_domain)\n",
    "\n",
    "# Redefine clean_text function to build_terms to return a list of tokens\n",
    "def build_terms(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    textos_limpios = [word for word in word_tokens if word not in stop_words and word.isalnum()]      \n",
    "    textos_limpios = [stemmer.stem(word) for word in textos_limpios]\n",
    "    return textos_limpios\n",
    "\n",
    "# Helper function to clean numeric fields \n",
    "def clean_numeric(value):\n",
    "    if isinstance(value, str):\n",
    "        value = re.sub(r'[^\\d.,]', '', value).replace(',', '')\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Apply cleaning to numeric columns\n",
    "for col in ['selling_price', 'actual_price', 'discount', 'average_rating']:\n",
    "    products[col] = products[col].apply(clean_numeric)\n",
    "\n",
    "# Assegurem que els NaN es converteixin en 0\n",
    "products['average_rating'] = products['average_rating'].fillna(0)\n",
    "\n",
    "# Apply build_terms function to the columns 'title' and 'description' of the products dataset\n",
    "products_cleaned = products.copy()\n",
    "\n",
    "products_cleaned['title'] = products_cleaned['title'].apply(build_terms)\n",
    "products_cleaned['description'] = products_cleaned['description'].apply(build_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3406c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define page_contents and title_index\n",
    "title_index = products['title'].to_dict()\n",
    "products['content_to_index'] = products['title'].fillna('') + ' ' + products['description'].fillna('')\n",
    "page_contents = products['content_to_index'].tolist()\n",
    "\n",
    "# Mappings between doc_ids and pids\n",
    "doc_id_to_pid = products['pid'].to_dict()\n",
    "pid_to_doc_id = {pid: i for i, pid in doc_id_to_pid.items()}\n",
    "\n",
    "N = len(page_contents) # Number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a60e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adapt our function create_index_tfidf from part 2 to return also the length of each document vector and the normalized term frequencies.\n",
    "def create_index_part3(documents_content, title_index_map, num_documents):\n",
    "\n",
    "    # Initialize data structures\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)\n",
    "    df = defaultdict(int)\n",
    "    idf = defaultdict(float)\n",
    "    title_index = defaultdict(str)\n",
    "    \n",
    "    doc_lengths = {} \n",
    "    \n",
    "    # Process each document\n",
    "    for page_id, content in enumerate(documents_content):\n",
    "        \n",
    "        # Build terms for the document\n",
    "        terms = build_terms(content)\n",
    "        \n",
    "        doc_lengths[page_id] = len(terms) \n",
    "\n",
    "        # Build index\n",
    "        title = title_index_map.get(page_id, \"No Title Found\")\n",
    "        title_index[page_id] = title    \n",
    "\n",
    "        \n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): \n",
    "            try:\n",
    "                current_page_index[term][1].append(position)\n",
    "            except KeyError:\n",
    "                current_page_index[term] = [page_id, array('I', [position])]\n",
    "                \n",
    "        # Normalize term frequencies\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        # Calculate tf normalized and DF\n",
    "        for term, posting in current_page_index.items():\n",
    "            if norm > 0:\n",
    "                tf[term].append(np.round(len(posting[1]) / norm, 4))\n",
    "            else:\n",
    "                tf[term].append(0.0)\n",
    "            \n",
    "            df[term] += 1 \n",
    "\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "    # Calculate IDF \n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(num_documents / df[term])), 4)\n",
    "\n",
    "    return index, tf, df, idf, title_index, doc_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3869b76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the TD-IDF index: 14.62 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execution of the index construction\n",
    "start_time = time.time()\n",
    "\n",
    "# Call the new function\n",
    "index, tf_norm, df, idf, title_index, doc_lengths = create_index_part3(page_contents, title_index, N)\n",
    "\n",
    "# Print total time taken\n",
    "print(\"Total time to create the TD-IDF index: {} seconds\" .format(np.round(time.time() - start_time, 2)))\n",
    "\n",
    "# Calculate average document length\n",
    "avg_doc_length = sum(doc_lengths.values()) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ce0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same function search_tfidf as in part 2, but isolated here for clarity\n",
    "def find_candidate_docs(query, index):\n",
    "    query_terms = build_terms(query)\n",
    "\n",
    "    if not query_terms:\n",
    "        return set()\n",
    "\n",
    "    candidate_docs = None \n",
    "    \n",
    "    for term in query_terms:\n",
    "        \n",
    "        term_docs = {posting[0] for posting in index[term]}\n",
    "            \n",
    "        if candidate_docs is None:\n",
    "            candidate_docs = term_docs\n",
    "        else:\n",
    "            candidate_docs &= term_docs \n",
    "\n",
    "    return candidate_docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efce2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = {\n",
    "        # Q1: Compulsory (validation_labels.csv)\n",
    "        \"q1\": \"women full sleeve sweatshirt cotton\",\n",
    "        \n",
    "        # Q2: Compulsory (validation_labels.csv)\n",
    "        \"q2\": \"men slim jeans blue\",\n",
    "        \n",
    "        # Q3: High Frequency Query (Based on Top DF)\n",
    "        \"q3\": \"neck solid fit\",\n",
    "\n",
    "        # Q4: Small Frequency Query (Based on Low DF)\n",
    "        \"q4\": \"trendiest glossi\",\n",
    "        \n",
    "        # Q5: Combined Query (User Simulation)\n",
    "        \"q5\": \"trendiest women\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d351a87",
   "metadata": {},
   "source": [
    "## 1. Rankings approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afdd76",
   "metadata": {},
   "source": [
    "### 1.1. TF-IDF Ranking + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0133c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_tfidf(query_terms, docs_to_rank, index, df, N):\n",
    "    # Build query vector\n",
    "    query_vector = {}\n",
    "    query_term_counts = collections.Counter(query_terms) \n",
    "    \n",
    "    # Build vector for the query\n",
    "    for term, count in query_term_counts.items():\n",
    "        if term not in df: continue\n",
    "            \n",
    "        # TF de la consulta\n",
    "        tf_q = 1 + math.log10(count)\n",
    "        \n",
    "        # IDF de la consulta \n",
    "        idf_q = math.log10(N / df[term])\n",
    "        \n",
    "        query_vector[term] = tf_q * idf_q\n",
    "\n",
    "    # Calculate document scores (Dot Product)\n",
    "    # Create a dictionary {doc_id: score}\n",
    "    doc_scores = defaultdict(float)\n",
    "    \n",
    "    # Iterate over each term in the QUERY (\n",
    "    for term, query_weight in query_vector.items():\n",
    "        # Calculate the weight of this term for the documents\n",
    "        idf_d = math.log10(N / df[term]) \n",
    "        \n",
    "        for posting in index[term]:\n",
    "            doc_id = posting[0]\n",
    "            \n",
    "            if doc_id in docs_to_rank:\n",
    "                \n",
    "                # Calculate Raw TF\n",
    "                raw_tf = len(posting[1]) \n",
    "                \n",
    "                # Calculate TF of the document (logarithmic)\n",
    "                tf_d = 1 + math.log10(raw_tf)\n",
    "                \n",
    "                doc_weight = tf_d * idf_d\n",
    "\n",
    "                # Accumulate the dot product (Q_weight * D_weight)\n",
    "                doc_scores[doc_id] += query_weight * doc_weight\n",
    "    \n",
    "    # Sort documents by score\n",
    "    ranked_docs = sorted(doc_scores.keys(), key=lambda d: doc_scores[d], reverse=True)\n",
    "    \n",
    "    scores_dict = {doc_id: doc_scores[doc_id] for doc_id in ranked_docs}\n",
    "    return ranked_docs, scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98cd42aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: q1\n",
      "Query: women full sleeve sweatshirt cotton\n",
      "Top 5 Ranked Document IDs: [4288, 4290, 24129, 25300, 23179]\n",
      "Scores: [4.957146787053152, 4.957146787053152, 4.92628436019739, 4.92628436019739, 4.853784875481479]\n",
      "--------------------------------------------------\n",
      "Query ID: q2\n",
      "Query: men slim jeans blue\n",
      "Top 5 Ranked Document IDs: [7595, 7605, 7617, 7623, 7634]\n",
      "Scores: [4.0298552985852965, 4.0298552985852965, 4.0298552985852965, 4.0298552985852965, 4.0298552985852965]\n",
      "--------------------------------------------------\n",
      "Query ID: q3\n",
      "Query: neck solid fit\n",
      "Top 5 Ranked Document IDs: [9808, 9810, 25250, 25251, 25281]\n",
      "Scores: [1.0420485082185222, 0.9794479497805191, 0.9383224938524526, 0.9383224938524526, 0.9383224938524526]\n",
      "--------------------------------------------------\n",
      "Query ID: q4\n",
      "Query: trendiest glossi\n",
      "Top 5 Ranked Document IDs: []\n",
      "Scores: []\n",
      "--------------------------------------------------\n",
      "Query ID: q5\n",
      "Query: trendiest women\n",
      "Top 5 Ranked Document IDs: [794, 821, 826, 852, 17869]\n",
      "Scores: [9.90637288146177, 9.90637288146177, 9.90637288146177, 9.90637288146177, 9.875510454606006]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Rank test queries\n",
    "for qid, query in test_queries.items():\n",
    "    query_terms = build_terms(query)\n",
    "    candidate_docs = find_candidate_docs(query, index)\n",
    "    ranked_docs, scores = rank_documents_tfidf(query_terms, candidate_docs, index, df, N)\n",
    "\n",
    "    print('Query ID:', qid)\n",
    "    print('Query:', query)\n",
    "    print('Top 5 Ranked Document IDs:', ranked_docs[:5])\n",
    "    print('Scores:', [scores[doc_id] for doc_id in ranked_docs[:5]])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa7141",
   "metadata": {},
   "source": [
    "**TF-IDF (Dot Product)**\n",
    "\n",
    "*Pros:*\n",
    "- Simple & Intuitive: The model is easy to understand and implement.\n",
    "- Good Baseline: It provides a solid, classic baseline to measure other models against.\n",
    "\n",
    "*Cons:*\n",
    "- Biased Towards Length: Favors longer documents, which is often not what a user wants.\n",
    "- No TF Saturation: Can be \"gamed\" by keyword-stuffing, rewarding documents that overuse a query term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc2cb03",
   "metadata": {},
   "source": [
    "### 1.2. BM25 Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eea6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BM25 parameters\n",
    "K1 = 1.2\n",
    "B = 0.75\n",
    "\n",
    "def rank_documents_bm25(query_terms, docs_to_rank, index, df, N, doc_lengths, avg_doc_length):\n",
    "\n",
    "    # Initialize document scores\n",
    "    doc_scores = defaultdict(float)\n",
    "    \n",
    "    # Precompute IDF for query terms\n",
    "    idf_cache = {}\n",
    "    for term in query_terms:\n",
    "        if term not in df: continue\n",
    "        # Fórmula IDF de BM25 (compte, logaritme natural)\n",
    "        df_term = df[term]\n",
    "        idf_cache[term] = math.log(1 + (N - df_term + 0.5) / (df_term + 0.5))\n",
    "\n",
    "    # Itearate over documents to rank\n",
    "    for doc_id in docs_to_rank:\n",
    "        doc_len = doc_lengths[doc_id] \n",
    "        \n",
    "        # Iterate over query terms\n",
    "        for term in query_terms:\n",
    "            if term not in idf_cache: continue \n",
    "                \n",
    "            # Obtain Raw TF for the term in the document\n",
    "            raw_tf = 0\n",
    "            for posting in index[term]:\n",
    "                if posting[0] == doc_id:\n",
    "                    raw_tf = len(posting[1]) \n",
    "                    break \n",
    "            \n",
    "            if raw_tf == 0: continue \n",
    "            \n",
    "            # BM25 TF component\n",
    "            tf_num = raw_tf * (K1 + 1)\n",
    "            tf_den = raw_tf + K1 * (1 - B + B * (doc_len / avg_doc_length))\n",
    "            tf_score = tf_num / tf_den\n",
    "            \n",
    "            # Scoring\n",
    "            doc_scores[doc_id] += idf_cache[term] * tf_score\n",
    "\n",
    "    # Sort documents by score\n",
    "    ranked_docs = sorted(doc_scores.keys(), key=lambda d: doc_scores[d], reverse=True)\n",
    "    scores_dict = {doc_id: doc_scores[doc_id] for doc_id in ranked_docs}\n",
    "    return ranked_docs, scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b89a8977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: q1\n",
      "Query: women full sleeve sweatshirt cotton\n",
      "Top 5 Ranked Document IDs: [4288, 4290, 14655, 25149, 25151]\n",
      "Scores: [12.100026652674897, 12.100026652674897, 11.889998681164478, 11.127435413300294, 10.922320440059151]\n",
      "--------------------------------------------------\n",
      "Query ID: q2\n",
      "Query: men slim jeans blue\n",
      "Top 5 Ranked Document IDs: [24544, 24547, 11292, 10283, 26174]\n",
      "Scores: [11.063364735088644, 11.01005550711725, 10.567155611301338, 10.567155611301338, 10.567155611301338]\n",
      "--------------------------------------------------\n",
      "Query ID: q3\n",
      "Query: neck solid fit\n",
      "Top 5 Ranked Document IDs: [12184, 12143, 12152, 12224, 24712]\n",
      "Scores: [4.392796330249203, 4.374840279049088, 4.374840279049088, 4.2867579498439055, 4.2229630784650825]\n",
      "--------------------------------------------------\n",
      "Query ID: q4\n",
      "Query: trendiest glossi\n",
      "Top 5 Ranked Document IDs: []\n",
      "Scores: []\n",
      "--------------------------------------------------\n",
      "Query ID: q5\n",
      "Query: trendiest women\n",
      "Top 5 Ranked Document IDs: [821, 794, 826, 852, 17877]\n",
      "Scores: [7.549528938396387, 7.549528938396387, 6.9611248664839005, 6.854377844127292, 5.9189284678438705]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Rank test queries\n",
    "for qid, query in test_queries.items():\n",
    "    query_terms = build_terms(query)\n",
    "    candidate_docs = find_candidate_docs(query, index)\n",
    "    ranked_docs_bm25, scores_bm25 = rank_documents_bm25(query_terms, candidate_docs, index, df, N, doc_lengths, avg_doc_length)\n",
    "\n",
    "    print('Query ID:', qid)\n",
    "    print('Query:', query)\n",
    "    print('Top 5 Ranked Document IDs:', ranked_docs_bm25[:5])\n",
    "    print('Scores:', [scores_bm25[doc_id] for doc_id in ranked_docs_bm25[:5]])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91267ca",
   "metadata": {},
   "source": [
    "**BM25**\n",
    "\n",
    "*Pros:*\n",
    "- State-of-the-Art: It is the industry-standard lexical ranking function for a reason, generally providing superior relevance.\n",
    "- Sophisticated Normalization: It intelligently balances TF saturation ($k_1$) and document length ($b$).\n",
    "\n",
    "*Cons:*\n",
    "- \"Black Box\" Parameters: Requires tuning $k_1$ and $b$ (we used standard defaults), which can be complex.\n",
    "- Computationally Heavier: Requires more pre-calculated data (specifically doc_lengths and avg_doc_length)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c008f4",
   "metadata": {},
   "source": [
    "### 1.3. Hybrid Ranking (your score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1157bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_your_score(ranked_docs_bm25, scores_bm25, products_df):\n",
    "    # Define weights\n",
    "    W_BM25 = 0.8  # 80% textual relevance\n",
    "    W_RATING = 0.2 # 20% quality of the product\n",
    "    \n",
    "    your_scores = {}\n",
    "    \n",
    "    max_bm25_score = next(iter(scores_bm25.values())) if scores_bm25 else 0\n",
    "    if max_bm25_score == 0: \n",
    "        max_bm25_score = 1 \n",
    "     \n",
    "    # Define maximum rating   \n",
    "    MAX_RATING = 5.0\n",
    "    \n",
    "    # Iterate over documents\n",
    "    for doc_id in ranked_docs_bm25:\n",
    "     \n",
    "        norm_bm25 = scores_bm25[doc_id] / max_bm25_score\n",
    "        \n",
    "        try:\n",
    "            rating = products_df.at[doc_id, 'average_rating']\n",
    "            if not isinstance(rating, (int, float)): rating = 0\n",
    "        except:\n",
    "            rating = 0\n",
    "        \n",
    "        # Normalize rating\n",
    "        norm_rating = rating / MAX_RATING\n",
    "        \n",
    "        # Calculate final score\n",
    "        final_score = (W_BM25 * norm_bm25) + (W_RATING * norm_rating)\n",
    "        \n",
    "        # Store final score\n",
    "        your_scores[doc_id] = final_score\n",
    "        \n",
    "    ranked_docs = sorted(your_scores.keys(), key=lambda d: your_scores[d], reverse=True)\n",
    "    scores_dict = {doc_id: your_scores[doc_id] for doc_id in ranked_docs}\n",
    "    \n",
    "    return ranked_docs, scores_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5073569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: q1\n",
      "Query: women full sleeve sweatshirt cotton\n",
      "Top 5 Ranked Document IDs: [4288, 4290, 14655, 25149, 25015]\n",
      "Scores: ['0.9880', '0.9880', '0.9781', '0.9277', '0.9221']\n",
      "--------------------------------------------------\n",
      "Query ID: q2\n",
      "Query: men slim jeans blue\n",
      "Top 5 Ranked Document IDs: [10303, 24544, 24547, 10401, 10348]\n",
      "Scores: ['0.9641', '0.9520', '0.9481', '0.9481', '0.9361']\n",
      "--------------------------------------------------\n",
      "Query ID: q3\n",
      "Query: neck solid fit\n",
      "Top 5 Ranked Document IDs: [21243, 21273, 14713, 13512, 24730]\n",
      "Scores: ['0.9449', '0.9449', '0.9357', '0.9348', '0.9331']\n",
      "--------------------------------------------------\n",
      "Query ID: q4\n",
      "Query: trendiest glossi\n",
      "Top 5 Ranked Document IDs: []\n",
      "Scores: []\n",
      "--------------------------------------------------\n",
      "Query ID: q5\n",
      "Query: trendiest women\n",
      "Top 5 Ranked Document IDs: [821, 794, 17877, 17869, 826]\n",
      "Scores: ['0.9600', '0.9600', '0.8192', '0.7498', '0.7376']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Rank test queries\n",
    "for qid, query in test_queries.items():\n",
    "    query_terms = build_terms(query)\n",
    "    candidate_docs = find_candidate_docs(query, index)\n",
    "    ranked_docs_bm25, scores_bm25 = rank_documents_bm25(query_terms, candidate_docs, index, df, N, doc_lengths, avg_doc_length)\n",
    "    ranked_docs_yourscore, scores_yourscore = rank_documents_your_score(ranked_docs_bm25, scores_bm25, products_cleaned)\n",
    "\n",
    "    print('Query ID:', qid)\n",
    "    print('Query:', query)\n",
    "    print('Top 5 Ranked Document IDs:', ranked_docs_yourscore[:5])\n",
    "    print('Scores:', [f\"{scores_yourscore[doc_id]:.4f}\" for doc_id in ranked_docs_yourscore[:5]])\n",
    "    print(\"-\" * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10246d4e",
   "metadata": {},
   "source": [
    "**Hybrid Ranking**\n",
    "\n",
    "*Pros:*\n",
    "- Smarter Tie-Breaker: When textual relevance is similar, the product with the better rating wins.\n",
    "- Better User Experience: Aligns with real-world user intent (users want good, relevant products, not just textually relevant ones).\n",
    "- Leverages Part 1 Work: Directly uses the average_rating field we identified and cleaned in Part 1.\n",
    "\n",
    "*Cons:*\n",
    "- Popularity Bias: This is the main drawback. New products with 0 ratings are unfairly penalized, making it very difficult for them to ever appear in the top results.\n",
    "- Arbitrary Weights: The 80/2s0 split is an educated guess. The optimal weights would need to be found experimentally (e.g., via a Grid Search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c128c64d",
   "metadata": {},
   "source": [
    "## 2. Word2vec + cosine ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb374d1",
   "metadata": {},
   "source": [
    "### 2.1. Training the Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a70fa51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus: 28080\n",
      "Example document tokens: ['solid', 'women', 'multicolor', 'track', 'pant', 'yorker', 'trackpant', 'rich', 'comb', 'cotton', 'give', 'rich', 'comfort', 'skin', 'friendli', 'fabric', 'waistband']\n"
     ]
    }
   ],
   "source": [
    "corpus = (products_cleaned[\"title\"] + products_cleaned[\"description\"]).tolist()\n",
    "\n",
    "print(\"Number of documents in corpus:\", len(corpus))\n",
    "print(\"Example document tokens:\", corpus[0][:20]) \n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=100,  \n",
    "    window=5,       \n",
    "    min_count=2,       \n",
    "    workers=4,        \n",
    "    sg=1,            \n",
    "    epochs=10         \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a9d22",
   "metadata": {},
   "source": [
    "### 2.2 Building text embeddings (average of word vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28787e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs with Word2Vec vector: 28080 of 28080\n"
     ]
    }
   ],
   "source": [
    "def terms_to_w2v_vector(terms, model):\n",
    "    vectors = []\n",
    "    for t in terms:\n",
    "        if t in model.wv:\n",
    "            vectors.append(model.wv[t])\n",
    "    if not vectors:\n",
    "        return None\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "doc_w2v_vectors = {}\n",
    "\n",
    "for doc_id in range(N):\n",
    "    terms = products_cleaned.loc[doc_id, \"title\"] + products_cleaned.loc[doc_id, \"description\"]\n",
    "    vec = terms_to_w2v_vector(terms, w2v_model)\n",
    "    if vec is not None:\n",
    "        doc_w2v_vectors[doc_id] = vec\n",
    "\n",
    "print(\"Docs with Word2Vec vector:\", len(doc_w2v_vectors), \"of\", N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db629a7f",
   "metadata": {},
   "source": [
    "### 2.3 Ranking documents with Word2Vec + cosine (top-20 for the 5 queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: q1\n",
      "Query: women full sleeve sweatshirt cotton\n",
      "Number of candidate docs: 215\n",
      "Top 20 doc IDs: [14655, 4288, 4290, 22995, 23042, 23044, 23046, 23054, 23056, 23060, 22856, 22869, 22892, 22924, 22985, 22990, 23006, 23015, 23021, 23029]\n",
      "------------------------------------------------------------\n",
      "Query ID: q2\n",
      "Query: men slim jeans blue\n",
      "Number of candidate docs: 176\n",
      "Top 20 doc IDs: [11292, 10283, 26174, 10303, 10308, 26184, 10313, 26186, 11339, 11350, 10348, 10391, 10401, 5797, 10415, 10416, 10417, 5827, 5828, 6858]\n",
      "------------------------------------------------------------\n",
      "Query ID: q3\n",
      "Query: neck solid fit\n",
      "Number of candidate docs: 742\n",
      "Top 20 doc IDs: [13512, 12990, 12992, 12988, 12184, 12989, 12223, 26052, 12143, 12208, 11403, 12104, 12224, 21243, 21270, 21334, 21352, 12222, 21762, 21802]\n",
      "------------------------------------------------------------\n",
      "Query ID: q4\n",
      "Query: trendiest glossi\n",
      "Number of candidate docs: 0\n",
      "Top 20 doc IDs: []\n",
      "------------------------------------------------------------\n",
      "Query ID: q5\n",
      "Query: trendiest women\n",
      "Number of candidate docs: 9\n",
      "Top 20 doc IDs: [821, 794, 826, 852, 17877, 17869, 17870, 17872, 17891]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(a, b):\n",
    "    denom = la.norm(a) * la.norm(b)\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def rank_documents_word2vec(query, docs_to_rank, model, doc_vectors):\n",
    "    query_terms = build_terms(query)\n",
    "    q_vec = terms_to_w2v_vector(query_terms, model)\n",
    "    if q_vec is None or not docs_to_rank:\n",
    "        return [], {}\n",
    "    \n",
    "    scores = {}\n",
    "    for doc_id in docs_to_rank:\n",
    "        d_vec = doc_vectors.get(doc_id)\n",
    "        if d_vec is None:\n",
    "            continue\n",
    "        scores[doc_id] = cosine_sim(q_vec, d_vec)\n",
    "    \n",
    "    ranked_docs = sorted(scores.keys(), key=lambda d: scores[d], reverse=True)\n",
    "    return ranked_docs, scores\n",
    "\n",
    "for qid, query in test_queries.items():\n",
    "    candidate_docs = find_candidate_docs(query, index)\n",
    "    ranked_docs_w2v, scores_w2v = rank_documents_word2vec(\n",
    "        query, candidate_docs, w2v_model, doc_w2v_vectors\n",
    "    )\n",
    "    print(\"Query ID:\", qid)\n",
    "    print(\"Query:\", query)\n",
    "    print(\"Number of candidate docs:\", len(candidate_docs))\n",
    "    print(\"Top 20 doc IDs:\", ranked_docs_w2v[:20])\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c0694",
   "metadata": {},
   "source": [
    "## 3. Can you imagine a better representation than word2vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e84f3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Word2Vec gives us a good starting point, but it is still quite simple: it learns a separate vector for each word and then we represent a document by just averaging those word vectors. In that process we lose information about document structure and some nuances of meaning.\n",
    "\n",
    "One possible improvement is **Doc2Vec**, which directly learns a vector for each document instead of only for individual words. In this way, the model can capture more global information about the document (topics, style, etc.) in a single embedding. The drawback is that Doc2Vec is usually harder to train and more sensitive to hyperparameters and data size than the simple “average of Word2Vec vectors”, so it is also more difficult to tune and debug in practice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
