{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614b1eca",
   "metadata": {},
   "source": [
    "# Part 2. Indexing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe57ac",
   "metadata": {},
   "source": [
    "Author/s: <font color=\"blue\">Jhonatan Barcos Gambaro | Daniel Alexander Yearwood</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">jhonatan.barcos01@estudiant.upf.edu | danielalexander.yearwood01@estudiant.upf.edu </font>\n",
    "\n",
    "Date: <font color=\"blue\">31/10/2025</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04c656cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe4afc",
   "metadata": {},
   "source": [
    "## 0. Data Preprocesing (Recap Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504ad43",
   "metadata": {},
   "source": [
    "For the implementation and development of this part of the project, we will only need to clean up the “title” and “description” variables at the textual level. Therefore, we will limit part 1 to only what is essential and necessary for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc2eea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "data_path = '../../data/fashion_products_dataset.json'\n",
    "products = pd.read_json(data_path)\n",
    "\n",
    "# Text Preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "# Define new stop words that depends on the domain of the data\n",
    "stop_words_domain = {\n",
    "    'made', 'india', 'proudly', 'use', 'year', 'round', \n",
    "    'look', 'design', 'qualiti', 'day', 'make',       \n",
    "    'feel', 'perfect', 'great', 'wash', 'style',      \n",
    "}\n",
    "stop_words = stop_words.union(stop_words_domain)\n",
    "\n",
    "# Redefine clean_text function to build_terms to return a list of tokens\n",
    "def build_terms(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    textos_limpios = [word for word in word_tokens if word not in stop_words and word.isalnum()]      \n",
    "    textos_limpios = [stemmer.stem(word) for word in textos_limpios]\n",
    "    return textos_limpios\n",
    "\n",
    "# Apply build_terms function to the columns 'title' and 'description' of the products dataset\n",
    "products_cleaned = products.copy()\n",
    "products_cleaned['title'] = products_cleaned['title'].apply(build_terms)\n",
    "products_cleaned['description'] = products_cleaned['description'].apply(build_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22ab38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creant estructures de dades (page_contents, title_index)...\n"
     ]
    }
   ],
   "source": [
    "# Define page_contents and title_index\n",
    "title_index = products['title'].to_dict()\n",
    "products['content_to_index'] = products['title'].fillna('') + ' ' + products['description'].fillna('')\n",
    "page_contents = products['content_to_index'].tolist()\n",
    "\n",
    "# Mappings between doc_ids and pids\n",
    "doc_id_to_pid = products['pid'].to_dict()\n",
    "pid_to_doc_id = {pid: i for i, pid in doc_id_to_pid.items()}\n",
    "\n",
    "N = len(page_contents) # Number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06932a84",
   "metadata": {},
   "source": [
    "## 1. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452393c",
   "metadata": {},
   "source": [
    "### 1.1. Build inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function adapted from Lab 1\n",
    "def create_index_tfidf(documents_content, title_index_map, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "\n",
    "    Argument:\n",
    "    documents_content -- collection of document contents\n",
    "    title_index_map -- mapping of document IDs to titles\n",
    "    num_documents -- total number of documents\n",
    "\n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)\n",
    "    df = defaultdict(int)\n",
    "    idf = defaultdict(float)\n",
    "    title_index = defaultdict(str)\n",
    "    \n",
    "    # process each document\n",
    "    for page_id, content in enumerate(documents_content):\n",
    "        \n",
    "        # build current page index\n",
    "        terms = build_terms(content)\n",
    "\n",
    "        # build current page index\n",
    "        title = title_index_map.get(page_id, \"No Title Found\")\n",
    "        title_index[page_id] = title    \n",
    "\n",
    "        ## ===============================================================\n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and its text is\n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "        ## the term ‘web’ appears in document 1 in positions 0,\n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "\n",
    "        \n",
    "        # initialize current page index\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): \n",
    "            try:\n",
    "                current_page_index[term][1].append(position)\n",
    "            except KeyError:\n",
    "                current_page_index[term] = [page_id, array('I', [position])]\n",
    "                \n",
    "        # normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document.\n",
    "            # posting ==> [current_doc, [list of positions]]\n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1]) / norm, 4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] += 1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "    # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "    # Note: It is computed later after we know the df.\n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(num_documents / df[term])), 4)\n",
    "\n",
    "\n",
    "    return index, tf, df, idf, title_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b1ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the TD-IDF index: 24.39 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execution of the index construction\n",
    "start_time = time.time()\n",
    "\n",
    "# Call the new function\n",
    "index, tf, df, idf, title_index = create_index_tfidf(page_contents, title_index, N)\n",
    "\n",
    "# Print total time taken\n",
    "print(\"Total time to create the TD-IDF index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14ce6b",
   "metadata": {},
   "source": [
    "### 1.2. Propose test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ca9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definides 5 consultes de prova:\n",
      "  q1: women full sleeve sweatshirt cotton\n",
      "  q2: men slim jeans blue\n",
      "  q3: yorker trackpants\n",
      "  q4: saree silk traditional\n",
      "  q5: black solid jacket\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf164ad9",
   "metadata": {},
   "source": [
    "### 1.3. Rank your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c43734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function adapted from Lab 1 for AND search\n",
    "\n",
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    Returns the list of documents that contain all of the query terms (conjunctive AND).\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = None\n",
    "    for term in query:\n",
    "        try:\n",
    "            term_docs = {posting[0] for posting in index[term]}\n",
    "            if docs is None:\n",
    "                docs = term_docs\n",
    "            else:\n",
    "                docs &= term_docs\n",
    "        except KeyError:\n",
    "            # If any term is not in the index, no document can match all terms\n",
    "            return [], []\n",
    "    docs = list(docs) if docs is not None else []\n",
    "    ranked_docs, doc_scores = rank_documents(query, docs, index, idf, tf, title_index)\n",
    "    return ranked_docs, doc_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4475b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Represent the query as a weighted tf-idf vector\n",
    "#Represent each document as a weighted tfidf vector\n",
    "#Compute the cosine similarity score for the\n",
    "#query vector and each document vector\n",
    "#Rank documents with respect to the query by score\n",
    "#Return the top K (e.g., K = 10) to the user\n",
    "\n",
    "# Function copied from Lab 1 for ranking documents based on TF-IDF\n",
    "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "\n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "\n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms\n",
    "    # The remaining elements would become 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query.\n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex] = query_terms_count[term] / query_norm * idf[term]\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26\n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  \n",
    "\n",
    "    # Calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "\n",
    "    doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_docs, doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57def56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: women full sleeve sweatshirt cotton):\n",
      "\n",
      "\n",
      "======================\n",
      "Top 10 results out of 215 for the searched query:\n",
      "\n",
      "page_id= 4290 - page_title: Full Sleeve Solid Women Sweatshirt\n",
      "page_id= 4288 - page_title: Full Sleeve Solid Women Sweatshirt\n",
      "page_id= 25149 - page_title: Full Sleeve Self Design Women Sweatshirt\n",
      "page_id= 25300 - page_title: Full Sleeve Solid Women Sweatshirt\n",
      "page_id= 14655 - page_title: Full Sleeve Solid Women Sweatshirt\n",
      "page_id= 25151 - page_title: Full Sleeve Color Block Women Sweatshirt\n",
      "page_id= 25015 - page_title: Full Sleeve Color Block Women Sweatshirt\n",
      "page_id= 22995 - page_title: Full Sleeve Color Block Women Sweatshirt\n",
      "page_id= 25142 - page_title: Full Sleeve Self Design, Color Block Women Sweatshirt\n",
      "page_id= 24129 - page_title: Full Sleeve Graphic Print Women Sweatshirt\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: women full sleeve sweatshirt cotton):\\n\")\n",
    "query = \"women full sleeve sweatshirt cotton\"\n",
    "ranked_docs, scores = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, title_index[d_id]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
