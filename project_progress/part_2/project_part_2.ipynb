{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614b1eca",
   "metadata": {},
   "source": [
    "# Part 2. Indexing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe57ac",
   "metadata": {},
   "source": [
    "Author/s: <font color=\"blue\">Jhonatan Barcos Gambaro | Daniel Alexander Yearwood</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">jhonatan.barcos01@estudiant.upf.edu | danielalexander.yearwood01@estudiant.upf.edu </font>\n",
    "\n",
    "Date: <font color=\"blue\">31/10/2025</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c656cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe4afc",
   "metadata": {},
   "source": [
    "## 0. Data Preprocesing (Recap Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504ad43",
   "metadata": {},
   "source": [
    "For the implementation and development of this part of the project, we will only need to clean up the “title” and “description” variables at the textual level. Therefore, we will limit part 1 to only what is essential and necessary for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2eea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "data_path = '../../data/fashion_products_dataset.json'\n",
    "products = pd.read_json(data_path)\n",
    "\n",
    "# Text Preprocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "# Define new stop words that depends on the domain of the data\n",
    "stop_words_domain = {\n",
    "    'made', 'india', 'proudly', 'use', 'year', 'round', \n",
    "    'look', 'design', 'qualiti', 'day', 'make',       \n",
    "    'feel', 'perfect', 'great', 'wash', 'style',      \n",
    "}\n",
    "stop_words = stop_words.union(stop_words_domain)\n",
    "\n",
    "# Redefine clean_text function to build_terms to return a list of tokens\n",
    "def build_terms(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    textos_limpios = [word for word in word_tokens if word not in stop_words and word.isalnum()]      \n",
    "    textos_limpios = [stemmer.stem(word) for word in textos_limpios]\n",
    "    return textos_limpios\n",
    "\n",
    "# Apply build_terms function to the columns 'title' and 'description' of the products dataset\n",
    "products_cleaned = products.copy()\n",
    "products_cleaned['title'] = products_cleaned['title'].apply(build_terms)\n",
    "products_cleaned['description'] = products_cleaned['description'].apply(build_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f22ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define page_contents and title_index\n",
    "title_index = products['title'].to_dict()\n",
    "products['content_to_index'] = products['title'].fillna('') + ' ' + products['description'].fillna('')\n",
    "page_contents = products['content_to_index'].tolist()\n",
    "\n",
    "# Mappings between doc_ids and pids\n",
    "doc_id_to_pid = products['pid'].to_dict()\n",
    "pid_to_doc_id = {pid: i for i, pid in doc_id_to_pid.items()}\n",
    "\n",
    "N = len(page_contents) # Number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06932a84",
   "metadata": {},
   "source": [
    "## 1. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452393c",
   "metadata": {},
   "source": [
    "### 1.1. Build inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518e0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function adapted from Lab 1\n",
    "def create_index_tfidf(documents_content, title_index_map, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "\n",
    "    Argument:\n",
    "    documents_content -- collection of document contents\n",
    "    title_index_map -- mapping of document IDs to titles\n",
    "    num_documents -- total number of documents\n",
    "\n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)\n",
    "    df = defaultdict(int)\n",
    "    idf = defaultdict(float)\n",
    "    title_index = defaultdict(str)\n",
    "    \n",
    "    # process each document\n",
    "    for page_id, content in enumerate(documents_content):\n",
    "        \n",
    "        # build current page index\n",
    "        terms = build_terms(content)\n",
    "\n",
    "        # build current page index\n",
    "        title = title_index_map.get(page_id, \"No Title Found\")\n",
    "        title_index[page_id] = title    \n",
    "\n",
    "        ## ===============================================================\n",
    "        ## create the index for the **current page** and store it in current_page_index\n",
    "        ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and its text is\n",
    "        ##\"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "        ## the term ‘web’ appears in document 1 in positions 0,\n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "\n",
    "        \n",
    "        # initialize current page index\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): \n",
    "            try:\n",
    "                current_page_index[term][1].append(position)\n",
    "            except KeyError:\n",
    "                current_page_index[term] = [page_id, array('I', [position])]\n",
    "                \n",
    "        # normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            # posting will contain the list of positions for current term in current document.\n",
    "            # posting ==> [current_doc, [list of positions]]\n",
    "            # you can use it to infer the frequency of current term.\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1]) / norm, 4)) ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] += 1 # increment DF for current term\n",
    "\n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "\n",
    "    # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "    # Note: It is computed later after we know the df.\n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(num_documents / df[term])), 4)\n",
    "\n",
    "\n",
    "    return index, tf, df, idf, title_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66b1ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the TD-IDF index: 21.71 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execution of the index construction\n",
    "start_time = time.time()\n",
    "\n",
    "# Call the new function\n",
    "index, tf, df, idf, title_index = create_index_tfidf(page_contents, title_index, N)\n",
    "\n",
    "# Print total time taken\n",
    "print(\"Total time to create the TD-IDF index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14ce6b",
   "metadata": {},
   "source": [
    "### 1.2. Propose test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39ca9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent terms by document frequency:\n",
      "women : 13434\n",
      "men : 13106\n",
      "neck : 12225\n",
      "solid : 9826\n",
      "print : 9226\n",
      "cotton : 8151\n",
      "fit : 7397\n",
      "casual : 6901\n",
      "comfort : 6712\n",
      "shirt : 5719\n",
      "\n",
      "Some rare terms by document frequency:\n",
      "trendiest : 21\n",
      "glossi : 21\n",
      "tone : 21\n",
      "delic : 21\n",
      "compress : 21\n",
      "push : 21\n",
      "repeat : 21\n",
      "domin : 21\n",
      "tast : 21\n",
      "higher : 21\n"
     ]
    }
   ],
   "source": [
    "# Print more frequent terms by document frequency\n",
    "sorted_df = dict(sorted(df.items(), key=lambda item: item[1], reverse=True))\n",
    "print('Most frequent terms by document frequency:')\n",
    "\n",
    "# Print the top 10 most frequent terms\n",
    "for term, freq in list(sorted_df.items())[:10]:\n",
    "    print(term, \":\", freq)\n",
    "    \n",
    "# Print more rare terms by document frequency\n",
    "sorted_df_rare = dict(sorted(df.items(), key=lambda item: item[1]))\n",
    "print('\\nSome rare terms by document frequency:')\n",
    "\n",
    "# Print some rare terms\n",
    "for term, freq in list(sorted_df_rare.items())[3000:3010]:\n",
    "    print(term, \":\", freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1263080",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = {\n",
    "        # Q1: Compulsory (validation_labels.csv)\n",
    "        \"q1\": \"women full sleeve sweatshirt cotton\",\n",
    "        \n",
    "        # Q2: Compulsory (validation_labels.csv)\n",
    "        \"q2\": \"men slim jeans blue\",\n",
    "        \n",
    "        # Q3: High Frequency Query (Based on Top DF)\n",
    "        \"q3\": \"neck solid fit\",\n",
    "\n",
    "        # Q4: Small Frequency Query (Based on Low DF)\n",
    "        \"q4\": \"trendiest glossi\",\n",
    "        \n",
    "        # Q5: Combined Query (User Simulation)\n",
    "        \"q5\": \"trendiest women\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf164ad9",
   "metadata": {},
   "source": [
    "### 1.3. Rank your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c43734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function adapted from Lab 1 for AND search\n",
    "\n",
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    Returns the list of documents that contain all of the query terms (conjunctive AND).\n",
    "    \"\"\"\n",
    "    query = build_terms(query)\n",
    "    docs = None\n",
    "    for term in query:\n",
    "        try:\n",
    "            term_docs = {posting[0] for posting in index[term]}\n",
    "            if docs is None:\n",
    "                docs = term_docs\n",
    "            else:\n",
    "                docs &= term_docs\n",
    "        except KeyError:\n",
    "            # If any term is not in the index, no document can match all terms\n",
    "            return [], []\n",
    "    docs = list(docs) if docs is not None else []\n",
    "    ranked_docs, doc_scores = rank_documents(query, docs, index, idf, tf, title_index)\n",
    "    return ranked_docs, doc_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4475b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Represent the query as a weighted tf-idf vector\n",
    "#Represent each document as a weighted tfidf vector\n",
    "#Compute the cosine similarity score for the\n",
    "#query vector and each document vector\n",
    "#Rank documents with respect to the query by score\n",
    "#Return the top K (e.g., K = 10) to the user\n",
    "\n",
    "# Function adapted from Lab 1 for ranking documents based on TF-IDF (Only fixed to return empty results when no docs found)\n",
    "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "\n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "\n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms\n",
    "    # The remaining elements would become 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query.\n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex] = query_terms_count[term] / query_norm * idf[term]\n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26\n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  \n",
    "\n",
    "    # Calculate the score of each doc\n",
    "    # compute the cosine similarity between queryVector and each docVector:\n",
    "\n",
    "    doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try another query.\")\n",
    "        return [], []   ## Added to fix infinite loop in case of no results\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_docs, doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57def56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: women full sleeve sweatshirt cotton):\n",
      "\n",
      "Processing test query q1: women full sleeve sweatshirt cotton\n",
      "\n",
      "======================\n",
      "Top 10 results out of 215 for the searched query:\n",
      "\n",
      "page_id= 4290 - page_title: Full Sleeve Solid Women Sweatshirt\n",
      "page_id= 4288 - page_title: Full Sleeve Solid Women Sweatshirt\n",
      "page_id= 25149 - page_title: Full Sleeve Self Design Women Sweatshirt\n",
      "page_id= 25300 - page_title: Full Sleeve Solid Women Sweatshirt\n",
      "page_id= 14655 - page_title: Full Sleeve Solid Women Sweatshirt\n",
      "page_id= 25151 - page_title: Full Sleeve Color Block Women Sweatshirt\n",
      "page_id= 25015 - page_title: Full Sleeve Color Block Women Sweatshirt\n",
      "page_id= 22995 - page_title: Full Sleeve Color Block Women Sweatshirt\n",
      "page_id= 25142 - page_title: Full Sleeve Self Design, Color Block Women Sweatshirt\n",
      "page_id= 24129 - page_title: Full Sleeve Graphic Print Women Sweatshirt\n",
      "\n",
      "\n",
      "\n",
      "Processing test query q2: men slim jeans blue\n",
      "\n",
      "======================\n",
      "Top 10 results out of 176 for the searched query:\n",
      "\n",
      "page_id= 26186 - page_title: Slim Men Blue Jeans\n",
      "page_id= 26184 - page_title: Slim Men Blue Jeans\n",
      "page_id= 26174 - page_title: Slim Men Blue Jeans\n",
      "page_id= 24435 - page_title: Slim Men Blue Jeans\n",
      "page_id= 24430 - page_title: Slim Men Blue Jeans\n",
      "page_id= 17157 - page_title: Slim Men Blue Jeans\n",
      "page_id= 16194 - page_title: Slim Men Blue Jeans\n",
      "page_id= 16173 - page_title: Slim Men Blue Jeans\n",
      "page_id= 14232 - page_title: Slim Men Blue Jeans\n",
      "page_id= 14167 - page_title: Slim Men Blue Jeans\n",
      "\n",
      "\n",
      "\n",
      "Processing test query q3: neck solid fit\n",
      "\n",
      "======================\n",
      "Top 10 results out of 742 for the searched query:\n",
      "\n",
      "page_id= 12152 - page_title: Solid Women Round Neck Green T-Shirt\n",
      "page_id= 24946 - page_title: Solid Women Round Neck Orange T-Shirt\n",
      "page_id= 24945 - page_title: Solid Men Round Neck Orange T-Shirt\n",
      "page_id= 24944 - page_title: Solid Women Round Neck Blue T-Shirt\n",
      "page_id= 24942 - page_title: Solid Men Round Neck Orange T-Shirt\n",
      "page_id= 24936 - page_title: Solid Women Round Neck Green T-Shirt\n",
      "page_id= 24929 - page_title: Solid Women Round Neck Yellow T-Shirt\n",
      "page_id= 24927 - page_title: Solid Women Round Neck Orange T-Shirt\n",
      "page_id= 24925 - page_title: Solid Men Round Neck Green T-Shirt\n",
      "page_id= 24923 - page_title: Solid Men Round Neck Grey T-Shirt\n",
      "\n",
      "\n",
      "\n",
      "Processing test query q4: trendiest glossi\n",
      "No results found, try another query.\n",
      "\n",
      "======================\n",
      "Top 10 results out of 0 for the searched query:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing test query q5: trendiest women\n",
      "\n",
      "======================\n",
      "Top 10 results out of 9 for the searched query:\n",
      "\n",
      "page_id= 821 - page_title: Women Kurta and Pyjama Set Pure Cotton\n",
      "page_id= 794 - page_title: Women Kurta and Pyjama Set Pure Cotton\n",
      "page_id= 826 - page_title: Women Kurta and Pyjama Set Jacquard\n",
      "page_id= 852 - page_title: Women Kurta and Pyjama Set Tussar Silk\n",
      "page_id= 17877 - page_title: Skinny Women Light Blue Jeans\n",
      "page_id= 17891 - page_title: Skinny Women Light Blue Jeans\n",
      "page_id= 17872 - page_title: Skinny Women Dark Blue Jeans\n",
      "page_id= 17870 - page_title: Skinny Women Dark Blue Jeans\n",
      "page_id= 17869 - page_title: Graphic Print Women Denim Black Denim Shorts\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: women full sleeve sweatshirt cotton):\\n\")\n",
    "\n",
    "for query_id, query in test_queries.items():\n",
    "    print(\"Processing test query {}: {}\".format(query_id, query))\n",
    "    ranked_docs, scores = search_tf_idf(query, index)\n",
    "    top = 10\n",
    "\n",
    "    print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "    for d_id in ranked_docs[:top]:\n",
    "        print(\"page_id= {} - page_title: {}\".format(d_id, title_index[d_id]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80575bc",
   "metadata": {},
   "source": [
    "## 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beadcb5b",
   "metadata": {},
   "source": [
    "### 2.1. -----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
